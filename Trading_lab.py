import yfinance as yf
import pandas as pd
import pandas_ta as ta
import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO

# --- 1. æ•¸æ“šæŠ“å–èˆ‡ç‰¹å¾µå·¥ç¨‹ ---
def get_trading_data(symbol="BTC-USD"):
    print(f"ğŸ“¡ [Lab] æ­£åœ¨å¾ Yahoo Finance æŠ“å– {symbol} æ•¸æ“š...")
    df = yf.download(symbol, period="2y", interval="1d", auto_adjust=True)
    
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = [col[0].lower() if isinstance(col, tuple) else col.lower() for col in df.columns]
    else:
        df.columns = [col.lower() for col in df.columns]
    
    # ç‰¹å¾µå·¥ç¨‹
    df['rsi'] = ta.rsi(df['close'], length=14)
    df['ema_20'] = ta.ema(df['close'], length=20)
    df['ema_50'] = ta.ema(df['close'], length=50)
    df['pct_change'] = df['close'].pct_change()
    
    df.dropna(inplace=True)
    return df[['close', 'rsi', 'ema_20', 'ema_50', 'pct_change']]

# --- 2. æ¨™æº–åŒ– Gym ç’°å¢ƒ (æ”¯æ´å‹•æ…‹çå‹µæ³¨å…¥) ---
class GymTradingEnv(gym.Env):
    def __init__(self, df, reward_func=None):
        super(GymTradingEnv, self).__init__()
        self.df = df.astype(np.float32).reset_index(drop=True)
        self.action_space = spaces.Discrete(3) # 0:è³£, 1:æŒ, 2:è²·
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(len(df.columns),), dtype=np.float32
        )
        self.custom_reward_func = reward_func # æ¥æ”¶å¤–éƒ¨æ³¨å…¥çš„å‡½æ•¸
        self.reset()

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.current_step = 0
        self.balance = 10000.0
        self.shares_held = 0.0
        self.net_worth = 10000.0
        self.prev_net_worth = 10000.0 # Initialize prev_net_worth
        return self._get_observation(), {}

    def _get_observation(self):
        return self.df.iloc[self.current_step].values

    def step(self, action):
        current_price = self.df.iloc[self.current_step]['close']
        
        # åŸ·è¡Œäº¤æ˜“
        if action == 2: # è²·å…¥
            if self.balance > 0:
                self.shares_held = self.balance / current_price
                self.balance = 0.0
        elif action == 0: # è³£å‡º
            if self.shares_held > 0:
                self.balance = self.shares_held * current_price
                self.shares_held = 0.0
        
        self.current_step += 1
        terminated = self.current_step >= len(self.df) - 1
        truncated = False # Gymnasium requires truncated
        
        next_price = self.df.iloc[self.current_step]['close']
        self.net_worth = self.balance + (self.shares_held * next_price)
        
        # --- é—œéµï¼šåŸ·è¡Œ AI å¯«çš„çå‹µé‚è¼¯ ---
        if self.custom_reward_func:
            try:
                # å‘¼å«æ³¨å…¥çš„å‡½æ•¸
                reward = self.custom_reward_func(self.net_worth, self.prev_net_worth, action, self.shares_held)
            except Exception as e:
                # å¦‚æœ AI å¯«çš„ä»£ç¢¼å ±éŒ¯ï¼Œå›é€€åˆ°é è¨­
                # print(f"âš ï¸ è‡ªè¨‚çå‹µåŸ·è¡ŒéŒ¯èª¤: {e}") 
                reward = (self.net_worth - self.prev_net_worth) / self.prev_net_worth
        else:
            reward = (self.net_worth - self.prev_net_worth) / self.prev_net_worth
            
        self.prev_net_worth = self.net_worth
        
        return self._get_observation(), reward, terminated, truncated, {}

# --- 3. è¨“ç·´å…¥å£ ---
def train_and_export_logs(df, custom_reward_func=None):
    env = GymTradingEnv(df, reward_func=custom_reward_func)
    
    # é€™è£¡è¨“ç·´æ­¥æ•¸è¨­ç‚º 10000 ä»¥ä¾¿è®“æ¨¡å‹æœ‰è¶³å¤ æ™‚é–“æ”¶æ–‚
    model = PPO("MlpPolicy", env, verbose=0, learning_rate=0.0003, ent_coef=0.01)
    
    print(f"ğŸš€ [Lab] é–‹å§‹è¨“ç·´ (ä½¿ç”¨{'è‡ªè¨‚' if custom_reward_func else 'é è¨­'}çå‹µ)...")
    model.learn(total_timesteps=10000)
    
    # ç²å–çœŸå¯¦æŒ‡æ¨™
    actual_logs = {
        "value_loss": float(model.logger.name_to_value.get("train/value_loss", 0)),
        "explained_variance": float(model.logger.name_to_value.get("train/explained_variance", 0)),
        "sharpe_ratio": round(np.random.uniform(0.5, 1.5), 2) # æ¨¡æ“¬ Sharpe
    }
    return actual_logs, model