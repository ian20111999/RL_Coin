import optuna
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.evaluation import evaluate_policy

# åŒ¯å…¥ä½ çš„å¯¦é©—å®¤
from Trading_lab import GymTradingEnv, get_trading_data

# å…¨å±€è®Šæ•¸ï¼šå¿«å–æ•¸æ“šï¼Œé¿å…æ¯æ¬¡ Trial éƒ½é‡æ–°ä¸‹è¼‰
CACHED_DF = None

def get_data():
    global CACHED_DF
    if CACHED_DF is None:
        CACHED_DF = get_trading_data() # ä¸‹è¼‰ä¸¦å¿«å–
    return CACHED_DF

def optimize_reward_logic(net_worth, prev_net_worth, action, shares_held, params):
    """
    é€™æ˜¯ä¸€å€‹ã€Œåƒæ•¸åŒ–ã€çš„çå‹µå‡½æ•¸ã€‚
    Optuna æœƒå‚³å…¥ params å­—å…¸ï¼Œå˜—è©¦ä¸åŒçš„æ•¸å€¼çµ„åˆã€‚
    """
    # 1. åŸºç¤æ”¶ç›Š (æ”¾å¤§å€ç‡ç”± Optuna æ±ºå®š)
    profit = (net_worth - prev_net_worth) / prev_net_worth
    reward = profit * params['profit_multiplier']
    
    # 2. æŒå€‰çå‹µ (é¼“å‹µæˆ–æ‡²ç½°æŒå€‰)
    if action == 1: # Hold
        reward += params['hold_reward']
        
    # 3. å›æ’¤æ‡²ç½° (å¦‚æœæ·¨å€¼ä¸‹è·Œï¼Œçµ¦äºˆé¡å¤–æ‡²ç½°)
    if net_worth < prev_net_worth:
        reward -= params['drawdown_penalty']
        
    return reward

def objective(trial):
    """
    Optuna çš„æ ¸å¿ƒè¿´åœˆï¼š
    1. å»ºè­°åƒæ•¸ -> 2. è¨“ç·´æ¨¡å‹ -> 3. å›å‚³åˆ†æ•¸
    """
    
    # --- A. å®šç¾©è¦å„ªåŒ–çš„è¶…åƒæ•¸ç©ºé–“ ---
    params = {
        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),
        'ent_coef': trial.suggest_float('ent_coef', 0.0, 0.1),
        'gamma': trial.suggest_categorical('gamma', [0.9, 0.95, 0.99]),
        # çå‹µå‡½æ•¸çš„åƒæ•¸
        'profit_multiplier': trial.suggest_float('profit_multiplier', 10.0, 200.0),
        'hold_reward': trial.suggest_float('hold_reward', -0.01, 0.01), # å¯ä»¥æ˜¯è² çš„(æ‡²ç½°)æˆ–æ­£çš„(çå‹µ)
        'drawdown_penalty': trial.suggest_float('drawdown_penalty', 0.0, 0.5)
    }
    
    # --- B. å»ºç«‹å¸¶æœ‰å‹•æ…‹çå‹µçš„ç’°å¢ƒ ---
    # ä½¿ç”¨ lambda å‡½å¼å°‡ params æ³¨å…¥åˆ°æˆ‘å€‘çš„çå‹µé‚è¼¯ä¸­
    df = get_data()
    
    # æˆ‘å€‘å®šç¾©ä¸€å€‹ wrapper è®“ç’°å¢ƒèƒ½å‘¼å«å¸¶åƒæ•¸çš„çå‹µå‡½æ•¸
    def current_reward_wrapper(nw, pnw, act, sh):
        return optimize_reward_logic(nw, pnw, act, sh, params)
        
    env = GymTradingEnv(df, reward_func=current_reward_wrapper)
    
    # --- C. è¨“ç·´æ¨¡å‹ (å¿«é€Ÿè©¦éŒ¯æ¨¡å¼) ---
    # é€™è£¡æ­¥æ•¸è¨­å°‘ä¸€é» (ä¾‹å¦‚ 5000-10000)ï¼Œç›®çš„æ˜¯å¿«é€Ÿç¯©é¸ï¼Œä¸ç”¨ç·´åˆ°å®Œç¾
    model = PPO("MlpPolicy", env, 
                verbose=0, 
                learning_rate=params['learning_rate'],
                ent_coef=params['ent_coef'],
                gamma=params['gamma'])
    
    try:
        model.learn(total_timesteps=5000)
    except Exception as e:
        print(f"Trial failed: {e}")
        return -float('inf') # è¨“ç·´å¤±æ•—çµ¦æ¥µä½åˆ†

    # --- D. è©•ä¼°æ¨¡å‹è¡¨ç¾ ---
    # æˆ‘å€‘ä¸çœ‹è¨“ç·´æ™‚çš„ Reward (å› ç‚ºé‚£è¢«æˆ‘å€‘æ”¹é)ï¼Œæˆ‘å€‘çœ‹ã€Œæœ€çµ‚æ·¨å€¼ã€æˆ–ã€Œå¤æ™®æ¯”ç‡ã€
    # é€™è£¡ç°¡å–®è·‘ä¸€æ¬¡å®Œæ•´çš„ episode ä¾†ç®—æœ€çµ‚æ·¨å€¼
    obs, _ = env.reset()
    done = False
    total_reward = 0
    
    # é‡æ–°è·‘ä¸€æ¬¡å›æ¸¬ç¢ºèªæˆæ•ˆ
    while not done:
        action, _ = model.predict(obs)
        obs, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        
    final_net_worth = env.net_worth
    
    # Optuna æœƒå˜—è©¦æœ€å¤§åŒ–é€™å€‹å›å‚³å€¼
    return final_net_worth

if __name__ == "__main__":
    print("ğŸš€ å•Ÿå‹• Optuna é‡åŒ–åƒæ•¸æœå°‹å¼•æ“...")
    
    # å»ºç«‹ Studyï¼Œç›®æ¨™æ˜¯æœ€å¤§åŒ– (maximize) æœ€çµ‚æ·¨å€¼
    study = optuna.create_study(direction="maximize")
    
    # é–‹å§‹è·‘ 20 è¼ªå¯¦é©— (ä½ å¯ä»¥éš¨æ„å¢åŠ )
    # n_jobs=1 ä»£è¡¨å–®ç·šç¨‹è·‘ (æ¯”è¼ƒç©©å®š)ï¼Œå¦‚æœä½ é›»è…¦å¼·å¯ä»¥è¨­ -1 (å…¨é€Ÿé‹è½‰)
    study.optimize(objective, n_trials=20, n_jobs=1)
    
    print("\n" + "="*50)
    print("ğŸ† æœå°‹å®Œæˆï¼æœ€ä½³åƒæ•¸çµ„åˆï¼š")
    print(study.best_params)
    print(f"ğŸ’° å°æ‡‰çš„æœ€çµ‚æ·¨å€¼: {study.best_value:.2f}")
    print("="*50)
    
    # ä½ å¯ä»¥æŠŠæœ€ä½³åƒæ•¸å­˜èµ·ä¾†ï¼Œä¹‹å¾Œç”¨ä¾†è¨“ç·´æœ€çµ‚æ¨¡å‹